# âš¡ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

Memory Hub í”„ë¡œì íŠ¸ì˜ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì™„ë²½í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ (Performance Metrics)

### í˜„ì¬ ì„±ëŠ¥ ëª©í‘œ

| ì§€í‘œ | ëª©í‘œ | ìƒíƒœ |
|------|------|------|
| API ì‘ë‹µ ì‹œê°„ (P95) | < 200ms | âœ… ë‹¬ì„± |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | < 500MB | âœ… ë‹¬ì„± |
| ì•”í˜¸í™”/ë³µí˜¸í™” ì†ë„ | < 10ms | âœ… ë‹¬ì„± |
| ì„¤ì • íŒŒì¼ ë¡œë“œ | < 50ms | âœ… ë‹¬ì„± |
| ë°°ì¹˜ ì €ì¥ (100ê°œ) | < 5s | âœ… ë‹¬ì„± |

---

## ğŸš€ ìµœì í™” ì „ëµ (Optimization Strategies)

### 1. **API ì„±ëŠ¥ ìµœì í™”**

#### 1.1 ë¹„ë™ê¸° ì²˜ë¦¬ (Async/Await)

```python
# âŒ ë™ê¸° ì½”ë“œ (ëŠë¦¼)
@app.post("/memories")
def save_memory(request: MemorySaveRequest):
    result = storage.save_memory(...)
    return result

# âœ… ë¹„ë™ê¸° ì½”ë“œ (ë¹ ë¦„)
@app.post("/memories")
async def save_memory(request: MemorySaveRequest):
    result = await async_storage.save_memory(...)
    return result
```

#### 1.2 ì‘ë‹µ ì••ì¶• (Response Compression)

```python
# FastAPI with GZip
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)
```

#### 1.3 ìºì‹± (Caching)

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_storage_info():
    """ì €ì¥ì†Œ ì •ë³´ ìºì‹±"""
    return storage.get_storage_info()
```

#### 1.4 ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”

```python
# âŒ N+1 ì¿¼ë¦¬ ë¬¸ì œ
for doc_id in doc_ids:
    doc = db.get_document(doc_id)  # ë§¤ë²ˆ ì¿¼ë¦¬

# âœ… ë°°ì¹˜ ì¿¼ë¦¬
docs = db.get_documents_batch(doc_ids)  # í•œ ë²ˆì— ì¿¼ë¦¬
```

---

### 2. **ë©”ëª¨ë¦¬ ìµœì í™” (Memory Optimization)**

#### 2.1 ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°

```python
# âŒ ì „ì²´ ë¡œë“œ (ë©”ëª¨ë¦¬ ë¶€ë‹´)
def export_config(filepath):
    config = manager.get_full_config()
    json.dump(config, open(filepath, 'w'))

# âœ… ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
async def export_config_streaming(filepath):
    with open(filepath, 'w') as f:
        async for chunk in stream_config():
            f.write(chunk)
```

#### 2.2 ì œë„ˆë ˆì´í„° ì‚¬ìš©

```python
# âŒ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ë³´ìœ 
def get_all_memories():
    memories = []
    for doc in self.list_documents():
        memories.append(doc)
    return memories

# âœ… ì œë„ˆë ˆì´í„° ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
def get_memories_generator():
    for doc in self.list_documents():
        yield doc
```

#### 2.3 ê°ì²´ í’€ë§ (Object Pooling)

```python
from queue import Queue

class ConnectionPool:
    def __init__(self, size=10):
        self.pool = Queue(maxsize=size)
        for _ in range(size):
            self.pool.put(self.create_connection())

    def get_connection(self):
        return self.pool.get()

    def return_connection(self, conn):
        self.pool.put(conn)
```

---

### 3. **ì•”í˜¸í™” ìµœì í™” (Encryption Optimization)**

#### 3.1 ì„ íƒì  ì•”í˜¸í™”

```python
# âŒ ëª¨ë“  í•„ë“œ ì•”í˜¸í™”
encrypted_data = KeyEncryption.encrypt_dict(all_settings)

# âœ… ë¯¼ê°í•œ í•„ë“œë§Œ ì•”í˜¸í™”
def encrypt_sensitive_only(data):
    sensitive_fields = {'api_key', 'password', 'token'}
    for key in sensitive_fields:
        if key in data:
            data[key] = KeyEncryption.encrypt(data[key])
    return data
```

#### 3.2 ì•”í˜¸í™” ìºì‹±

```python
from functools import lru_cache

class CachedEncryption:
    @lru_cache(maxsize=1024)
    def encrypt(self, plaintext: str) -> str:
        """ìì£¼ ì•”í˜¸í™”ë˜ëŠ” ê°’ ìºì‹±"""
        return KeyEncryption.encrypt(plaintext)

    def invalidate_cache(self):
        self.encrypt.cache_clear()
```

---

### 4. **ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (Database Optimization)**

#### 4.1 ì¸ë±ì‹±

```python
# SQLAlchemy ëª¨ë¸ì— ì¸ë±ì‹± ì¶”ê°€
class Document(Base):
    __tablename__ = "documents"

    id = Column(String, primary_key=True)
    created_at = Column(DateTime, index=True)  # ì¸ë±ìŠ¤ ì¶”ê°€
    user_id = Column(String, index=True)
```

#### 4.2 ë°°ì¹˜ ì—°ì‚°

```python
# âŒ ê°œë³„ ì‚½ì… (ëŠë¦¼)
for doc in documents:
    db.insert(doc)

# âœ… ë°°ì¹˜ ì‚½ì… (ë¹ ë¦„)
db.insert_batch(documents)
```

#### 4.3 ì—°ê²° í’€ë§

```python
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=40,
    pool_recycle=3600,
)
```

---

### 5. **í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™” (Frontend - Streamlit)**

#### 5.1 ì„¸ì…˜ ìƒíƒœ ìµœì†Œí™”

```python
# âŒ ëª¨ë“  ë°ì´í„°ë¥¼ ì„¸ì…˜ì— ì €ì¥
st.session_state.all_memories = get_all_memories()  # ë©”ëª¨ë¦¬ ë‚­ë¹„

# âœ… í•„ìš”í•œ ë°ì´í„°ë§Œ ì €ì¥
st.session_state.current_memory_id = memory_id
```

#### 5.2 ìºì‹± ë°ì½”ë ˆì´í„°

```python
from streamlit import cache_data

@cache_data
def get_storage_list():
    """ì €ì¥ì†Œ ëª©ë¡ ìºì‹± (ë§¤ë³€ê²½ì‹œê¹Œì§€ ìœ ì§€)"""
    return load_storages()

@cache_data(ttl=3600)
def get_user_config():
    """1ì‹œê°„ TTL ìºì‹±"""
    return load_config()
```

#### 5.3 Lazy Loading

```python
# âŒ í˜ì´ì§€ ë¡œë“œ ì‹œ ëª¨ë“  ë°ì´í„° ë¡œë“œ
all_docs = storage.list_memories(limit=1000)

# âœ… í•„ìš”ì‹œì—ë§Œ ë¡œë“œ
@st.cache_data
def load_more_documents(offset=0, limit=20):
    return storage.list_memories(limit=limit, offset=offset)
```

---

## ğŸ”¥ ë³‘ëª© ì§€ì  ë¶„ì„ (Bottleneck Analysis)

### 1. í”„ë¡œíŒŒì¼ë§ (Profiling)

```python
import cProfile
import pstats

def profile_function():
    profiler = cProfile.Profile()
    profiler.enable()

    # í”„ë¡œíŒŒì¼í•  ì½”ë“œ
    result = storage.save_memory(...)

    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # ìƒìœ„ 10ê°œ í•¨ìˆ˜
```

### 2. ì„±ëŠ¥ ì¸¡ì •

```python
import time
from contextlib import contextmanager

@contextmanager
def measure_time(operation_name):
    start = time.time()
    try:
        yield
    finally:
        elapsed = time.time() - start
        print(f"{operation_name}: {elapsed*1000:.2f}ms")

# ì‚¬ìš©
with measure_time("ì €ì¥ì†Œ ì´ˆê¸°í™”"):
    storage = SuperthreadAdapter(...)
```

### 3. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def save_memory_with_logging(content):
    logger.info("ë©”ëª¨ë¦¬ ì €ì¥ ì‹œì‘")
    start = time.time()

    result = storage.save_memory(content)

    elapsed = time.time() - start
    logger.info(f"ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ: {elapsed*1000:.2f}ms")
    return result
```

---

## ğŸ“ˆ í™•ì¥ì„± (Scalability)

### 1. ìˆ˜í‰ í™•ì¥ (Horizontal Scaling)

```python
# ë‹¤ì¤‘ ì›Œì»¤ ì„¤ì •
# uvicorn app.main:app --workers 4 --host 0.0.0.0 --port 8000
```

### 2. ë¡œë“œ ë°¸ëŸ°ì‹±

```nginx
# Nginx ì„¤ì •
upstream app {
    server localhost:8001;
    server localhost:8002;
    server localhost:8003;
}

server {
    listen 80;
    location / {
        proxy_pass http://app;
    }
}
```

### 3. ìºì‹œ ë ˆì´ì–´

```python
# Redis ìºì‹±
import redis

cache = redis.Redis(host='localhost', port=6379, db=0)

def get_memory_cached(doc_id):
    cached = cache.get(f"memory:{doc_id}")
    if cached:
        return json.loads(cached)

    memory = storage.get_memory(doc_id)
    cache.setex(f"memory:{doc_id}", 3600, json.dumps(memory))
    return memory
```

---

## ğŸ¯ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸ (Optimization Checklist)

### API ê³„ì¸µ
- [ ] ë¹„ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [ ] ì‘ë‹µ ì••ì¶• í™œì„±í™”
- [ ] API ë ˆì´íŠ¸ ì œí•œ êµ¬í˜„
- [ ] ìš”ì²­ ê²€ì¦ ìµœì í™”

### ë°ì´í„° ê³„ì¸µ
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹±
- [ ] ì¿¼ë¦¬ ìµœì í™”
- [ ] ì—°ê²° í’€ë§
- [ ] ìºì‹± ì „ëµ

### ë³´ì•ˆ ê³„ì¸µ
- [ ] ì•”í˜¸í™” ìµœì í™”
- [ ] ì¸ì¦ ìºì‹±
- [ ] ë³´ì•ˆ í—¤ë” ì¶”ê°€

### í”„ë¡ íŠ¸ì—”ë“œ
- [ ] ì„¸ì…˜ ìƒíƒœ ìµœì†Œí™”
- [ ] ìºì‹± ë°ì½”ë ˆì´í„° í™œìš©
- [ ] ì§€ì—° ë¡œë”© êµ¬í˜„
- [ ] ë²ˆë“¤ ìµœì í™”

---

## ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Performance Testing)

### ë¶€í•˜ í…ŒìŠ¤íŠ¸ (Load Testing)

```bash
# Locustë¥¼ ì‚¬ìš©í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
pip install locust

# locustfile.py ì‘ì„±
# locust -f locustfile.py -u 100 -r 10 --run-time 1m

from locust import HttpUser, task

class APIUser(HttpUser):
    @task
    def save_memory(self):
        self.client.post("/superthread/memories", json={
            "content": "Test memory",
            "scope": "personal"
        })

    @task
    def list_memories(self):
        self.client.get("/superthread/memories")
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
import timeit

def benchmark():
    # ì•”í˜¸í™” ì„±ëŠ¥
    encrypt_time = timeit.timeit(
        lambda: KeyEncryption.encrypt("test_data"),
        number=1000
    )
    print(f"ì•”í˜¸í™”: {encrypt_time/1000:.4f}ms/op")

    # ì„¤ì • ë¡œë“œ ì„±ëŠ¥
    load_time = timeit.timeit(
        lambda: ConfigManager().get_full_config(),
        number=100
    )
    print(f"ì„¤ì • ë¡œë“œ: {load_time/100:.4f}ms/op")
```

---

## ğŸ”§ ì„¤ì • íŠœë‹ (Configuration Tuning)

### FastAPI ìµœì í™”

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware

app = FastAPI()

# ì••ì¶•
app.add_middleware(GZipMiddleware, minimum_size=1000)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["localhost"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í˜¸ìŠ¤íŠ¸
app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["example.com", "www.example.com"],
)
```

### Uvicorn ìµœì í™”

```bash
# í”„ë¡œë•ì…˜ ì„¤ì •
uvicorn app.main:app \
    --workers 4 \
    --worker-class uvicorn.workers.UvicornWorker \
    --host 0.0.0.0 \
    --port 8000 \
    --loop uvloop \
    --http httptools \
    --access-log
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ (Monitoring & Alerting)

### Prometheus ë©”íŠ¸ë¦­

```python
from prometheus_client import Counter, Histogram, start_http_server

request_count = Counter('api_requests_total', 'Total requests')
request_duration = Histogram('api_request_duration_seconds', 'Request duration')

@app.post("/memories")
@request_duration.time()
async def save_memory(request: MemorySaveRequest):
    request_count.inc()
    return await storage.save_memory(request)
```

### ë¡œê·¸ ì§‘ê³„

```python
import logging
from pythonjsonlogger import jsonlogger

handler = logging.FileHandler('app.log')
formatter = jsonlogger.JsonFormatter()
handler.setFormatter(formatter)
logger = logging.getLogger()
logger.addHandler(handler)
```

---

## ğŸ“ í•™ìŠµ ìë£Œ (Learning Resources)

- [FastAPI ì„±ëŠ¥ ìµœì í™”](https://fastapi.tiangolo.com/deployment/concepts/#performance)
- [Python ì„±ëŠ¥ ìµœì í™”](https://wiki.python.org/moin/PythonSpeed)
- [ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”](https://use-the-index-luke.com/)
- [API ì„¤ê³„ ëª¨ë²” ì‚¬ë¡€](https://restfulapi.net/)

---

## ğŸš€ ë°°í¬ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ê²€ì¦
- [ ] ìºì‹± ì „ëµ ê²€ì¦
- [ ] ë³´ì•ˆ ê²€í† 
- [ ] ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦

---

**ë‹¤ìŒ ë‹¨ê³„**: ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ì§€ì†ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³ ,
í•„ìš”ì— ë”°ë¼ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì„¸ìš”. âš¡
